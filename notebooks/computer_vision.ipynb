{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pyserial in ./.venv/lib/python3.12/site-packages (3.5)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.12/site-packages (from opencv-python) (2.2.2)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN THIS ONCE. Uncomment right command for your OS before running.\n",
    "# !source .venv/bin/activate (MacOS / Linux)\n",
    "# !.venv\\Scripts\\activate (Windows)\n",
    "!pip3 install torch torchvision\n",
    "!pip install numpy\n",
    "!pip install pyserial\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Actual Code Functionality\n",
    "\n",
    "**ADVICE:** Follow the comments, not the code. ML libraries are highly abstracted, so I doubt you'd intuitively grasp the logic by reading the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-30 08:17:35.787 Python[10459:132945] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if your camera is working (ignore deprecation warning)\n",
    "!python3 -c \"import cv2; cap = cv2.VideoCapture(1); print(cap.isOpened()); cap.release()\"\n",
    "\n",
    "# If True, proceed. If not, try:\n",
    "# 1. run !system_profiler SPCameraDataType to list existing cameras\n",
    "# 2. Go to system preferences -> privacy & security -> camera -> check if your IDE has access to camera\n",
    "# 3. Try play around with the indexes e.g., cv2.VideoCapture(0)\n",
    "# FYI: cv.VideoCapture(1) doesn't need a proxy like Photobooth. It takes images straight from your camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.11.0\n",
      "OS: Darwin 23.6.0\n",
      "Available cameras:\n",
      " Camera:\n",
      "\n",
      "    FaceTime HD Camera:\n",
      "\n",
      "      Model ID: FaceTime HD Camera\n",
      "      Unique ID: 3F45E80A-0176-46F7-B185-BB9E2C0E82E3\n",
      "\n",
      "    iPhone (2) Camera:\n",
      "\n",
      "      Model ID: iPhone15,4\n",
      "      Unique ID: 38183445-CB3B-41DE-B949-417000000001\n",
      "\n",
      "\n",
      "\n",
      "Trying camera index: 0\n",
      "\n",
      "Trying camera index: 1\n",
      "Success with camera 1\n",
      "Frame stats - min: 0, max: 255, mean: 125.90424012988683\n"
     ]
    }
   ],
   "source": [
    "# Only use this code if you want to check which camera index works\n",
    "import cv2\n",
    "import time\n",
    "import platform\n",
    "\n",
    "def print_camera_info():\n",
    "    # List all available cameras on macOS\n",
    "    import subprocess\n",
    "    result = subprocess.run(['system_profiler', 'SPCameraDataType'], capture_output=True, text=True)\n",
    "    print(\"Available cameras:\\n\", result.stdout)\n",
    "\n",
    "try:\n",
    "    print(f\"OpenCV version: {cv2.__version__}\")\n",
    "    print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "    print_camera_info()\n",
    "    \n",
    "    # Try both built-in and external camera indices\n",
    "    for camera_index in [0, 1, -1]:\n",
    "        print(f\"\\nTrying camera index: {camera_index}\")\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        \n",
    "        if cap.isOpened():\n",
    "            # Set camera properties before reading\n",
    "            cap.set(cv2.CAP_PROP_CONVERT_RGB, 1.0)\n",
    "            cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M','J','P','G'))\n",
    "            \n",
    "            time.sleep(2)  # Warm up\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            if ret and frame is not None:\n",
    "                print(f\"Success with camera {camera_index}\")\n",
    "                print(f\"Frame stats - min: {frame.min()}, max: {frame.max()}, mean: {frame.mean()}\")\n",
    "                break\n",
    "        cap.release()\n",
    "    \n",
    "finally:\n",
    "    if 'cap' in locals():\n",
    "        cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 09:14:25,749 - INFO - Camera is opened: True\n",
      "2025-01-30 09:14:25,750 - INFO - Frame Width: 1920.0\n",
      "2025-01-30 09:14:25,751 - INFO - Frame Height: 1080.0\n",
      "2025-01-30 09:14:25,751 - INFO - FPS: 15.0\n",
      "2025-01-30 09:14:28,784 - INFO - Frame captured: True on attempt 1\n",
      "2025-01-30 09:14:28,785 - INFO - Frame shape: (1080, 1920, 3)\n",
      "2025-01-30 09:14:28,788 - INFO - Frame min value: 0\n",
      "2025-01-30 09:14:28,789 - INFO - Frame max value: 255\n",
      "2025-01-30 09:14:28,796 - INFO - Frame mean value: 115.30855725951646\n",
      "2025-01-30 09:14:28,808 - INFO - Frame captured and saved as test_capture.jpg\n",
      "2025-01-30 09:14:29,057 - INFO - prediction 1: plastic bag (728) -> 0.391\n",
      "2025-01-30 09:14:29,058 - INFO - prediction 2: motor scooter (670) -> 0.021\n",
      "2025-01-30 09:14:29,058 - INFO - prediction 3: snowmobile (802) -> 0.015\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Init preprocess method\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Use logging to classify errors and info messages\n",
    "import logging\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Before recording, close pre-existing camera connections (Fixed frame capture bug)\n",
    "pre_cap = cv2.VideoCapture(1)\n",
    "pre_cap.release()\n",
    "del pre_cap\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "try:\n",
    "    # Open camera and capture image\n",
    "    cap = cv2.VideoCapture(1) # 0 is your computer's default camera\n",
    "    # Configure auto exposure, brightness and contrast when you come to it.\n",
    "    logging.info(f\"Camera is opened: {cap.isOpened()}\")\n",
    "    # Print camera properties to debug\n",
    "    logging.info(f\"Frame Width: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}\")\n",
    "    logging.info(f\"Frame Height: {cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}\")\n",
    "    logging.info(f\"FPS: {cap.get(cv2.CAP_PROP_FPS)}\")\n",
    "    time.sleep(2)\n",
    "    if not cap.isOpened():\n",
    "         raise RuntimeError(\"Cannot open camera - check if camera is connected\")\n",
    "\n",
    "    # Have multiple capture attempts\n",
    "    for i in range(10):\n",
    "        ret, frame = cap.read() # ret: bool, frame: np.ndarray (matrix of pixels)\n",
    "        time.sleep(1)\n",
    "        if ret:\n",
    "            logging.info(f\"Frame captured: {ret} on attempt {i+1}\")\n",
    "            logging.info(f\"Frame shape: {frame.shape if frame is not None else 'No frame'}\")\n",
    "            # indicators whether frame is black\n",
    "            logging.info(f\"Frame min value: {frame.min()}\")  # Should not be 0\n",
    "            logging.info(f\"Frame max value: {frame.max()}\")  # Should not be 0\n",
    "            logging.info(f\"Frame mean value: {frame.mean()}\")  # Should not be 0\n",
    "            break\n",
    "        else:\n",
    "            logging.info(f\"Failed to capture frame on attempt {i+1}\")\n",
    "\n",
    "    # Look for 'test_capture.jpg' in the notebooks folder\n",
    "    cv2.imwrite(\"test_capture.jpg\", frame)\n",
    "    logging.info(\"Frame captured and saved as test_capture.jpg\")\n",
    "\n",
    "    # Pass frame through reprocess and then classify frame\n",
    "    input_tensor = preprocess(frame).unsqueeze(0)  # Add batch dimension e.g., [height, width, channels] -> [batch_size, h, w, c] [1, ...] means process 1 image at a time.\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Camera error: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    cap.release() # release camera resources after use\n",
    "\n",
    "# MobileNetV2 uses ImageNet labels get via HTTP request\n",
    "from urllib.request import urlopen\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "labels = urlopen(LABELS_URL).read().decode('utf-8').splitlines()\n",
    "\n",
    "with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        class_id = torch.argmax(output).item()  # Get predicted class ID\n",
    "\n",
    "        # Softmax with numerator = e**(p-max(p)) for each p\n",
    "        # softmax: list = [ numerator / sum(numerators) for each p ]\n",
    "        # sum(softmax) = 1.0 so we can express confidence score as a % (0-100)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0) # normalises into array of positive probability values\n",
    "        top3_prob, top3_catid = torch.topk(probabilities, 3)\n",
    "        for i in range(3):\n",
    "            # Interestingly detected objects on my face (glasses) instead of my face (with low confidence)\n",
    "            logging.info(f\"prediction {i+1}: {labels[top3_catid[i]]} ({top3_catid[i]}) -> {top3_prob[i].item():.3f}\")\n",
    "\n",
    "        # Calculate prediction accuracy (%)\n",
    "        true_label_id = 728 # Rigt now plastic bag but change to what it actually is\n",
    "\n",
    "        # single prediction accuracy\n",
    "        single_accuracy = 100 if true_label_id == top3_catid[0] else 0\n",
    "        logging.info(f\"Single prediction accuracy: {single_accuracy:.2f}%\")\n",
    "\n",
    "        # top-3 prediction accuracy (not binary, has ranked score)\n",
    "        top3_accuracy = 0\n",
    "        # I guess ranking logic right now is p * (1/2)**(i-1)\n",
    "        if true_label_id == top3_catid[0]:\n",
    "            top3_accuracy = 100\n",
    "        elif true_label_id == top3_catid[1]:\n",
    "            top3_accuracy = 50\n",
    "        elif true_label_id == top3_catid[2]:\n",
    "            top3_accuracy = 25\n",
    "        logging.info(f\"Top-3 prediction accuracy: {top3_accuracy:.2f}%\")\n",
    "\n",
    "        # confidence score\n",
    "        accuracy = top3_prob[true_label_id].item() * 100 if true_label_id in top3_catid else 0\n",
    "        logging.info(f\"Confidence score: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft Code for Serial Communication with Arduino\n",
    "\n",
    "```python\n",
    "import serial\n",
    "\n",
    "# Initialize serial communication\n",
    "arduino = serial.Serial('COM3', 9600)  # Adjust port and baud rate\n",
    "\n",
    "# Capture image\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Preprocess and classify\n",
    "input_tensor = preprocess(frame).unsqueeze(0)  # Add batch dimension\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "class_id = torch.argmax(output).item()  # Get predicted class ID\n",
    "\n",
    "# Send signal to Arduino\n",
    "arduino.write(str(class_id).encode())\n",
    "\n",
    "cap.release()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
